# Modeldy è®­ç»ƒä¸ä¼˜åŒ–å™¨å®Œæ•´æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç» Modeldy çš„è®­ç»ƒæ¥å£ã€ä¼˜åŒ–å™¨ç³»ç»Ÿä»¥åŠ CPU/CUDA åŒå®ç°æ¶æ„ã€‚

---

## ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [ä¼˜åŒ–å™¨](#ä¼˜åŒ–å™¨)
3. [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
4. [API è¯´æ˜](#api-è¯´æ˜)
5. [é«˜çº§ç”¨æ³•](#é«˜çº§ç”¨æ³•)
6. [CPU/CUDA åŒå®ç°æ¶æ„](#cpucuda-åŒå®ç°æ¶æ„)
7. [CUDA Kernel å®ç°æŒ‡å—](#cuda-kernel-å®ç°æŒ‡å—)
8. [ä¼˜åŒ–å»ºè®®](#ä¼˜åŒ–å»ºè®®)
9. [ç¼–è¯‘ä¸æµ‹è¯•](#ç¼–è¯‘ä¸æµ‹è¯•)

---

## æ¦‚è¿°

Modeldy æä¾›äº†å®Œæ•´çš„è®­ç»ƒæ¥å£ï¼ŒåŒ…æ‹¬å¤šç§ä¼˜åŒ–å™¨å’Œç®€å•çš„è®­ç»ƒå¾ªç¯ APIã€‚ä¼˜åŒ–å™¨ç³»ç»Ÿæ”¯æŒ CPU å’Œ CUDA ä¸¤ç§å®ç°ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹å‚æ•°ç±»å‹å¹¶è°ƒç”¨ç›¸åº”çš„ä¼˜åŒ–ç®—æ³•ã€‚

**æ ¸å¿ƒç‰¹æ€§ï¼š**
- ğŸš€ ä¸‰ç§ä¸»æµä¼˜åŒ–å™¨ï¼šSGDã€Adamã€RMSprop
- ğŸ”„ è‡ªåŠ¨ CPU/CUDA è®¾å¤‡æ£€æµ‹
- ğŸ“¦ ç®€å•æ˜“ç”¨çš„è®­ç»ƒ API
- âš¡ å®Œæ•´çš„ CUDA åŠ é€Ÿæ¡†æ¶

---

## ä¼˜åŒ–å™¨

### å¯ç”¨çš„ä¼˜åŒ–å™¨

#### 1. SGD (éšæœºæ¢¯åº¦ä¸‹é™)

```cpp
modeldy::SGD<float> optimizer(
    0.01f,      // learning_rate
    0.0f,       // momentum (å¯é€‰ï¼Œé»˜è®¤0)
    0.0f        // weight_decay (L2æ­£åˆ™åŒ–ï¼Œå¯é€‰ï¼Œé»˜è®¤0)
);
```

**ç‰¹ç‚¹ï¼š**
- ç®€å•é«˜æ•ˆ
- æ”¯æŒåŠ¨é‡åŠ é€Ÿ
- æ”¯æŒæƒé‡è¡°å‡ï¼ˆL2æ­£åˆ™åŒ–ï¼‰
- CPU å’Œ CUDA å®ç°éƒ½å·²å°±ç»ª

**æ¨èå­¦ä¹ ç‡ï¼š** 0.001 ~ 0.1

#### 2. Adam

```cpp
modeldy::Adam<float> optimizer(
    0.001f,     // learning_rate
    0.9f,       // beta1 (ä¸€é˜¶çŸ©ä¼°è®¡çš„æŒ‡æ•°è¡°å‡ç‡ï¼Œå¯é€‰)
    0.999f,     // beta2 (äºŒé˜¶çŸ©ä¼°è®¡çš„æŒ‡æ•°è¡°å‡ç‡ï¼Œå¯é€‰)
    1e-8f,      // epsilon (æ•°å€¼ç¨³å®šæ€§ï¼Œå¯é€‰)
    0.0f        // weight_decay (å¯é€‰)
);
```

**ç‰¹ç‚¹ï¼š**
- è‡ªé€‚åº”å­¦ä¹ ç‡
- å¯¹è¶…å‚æ•°ä¸æ•æ„Ÿ
- é€‚åˆå¤§å¤šæ•°æ·±åº¦å­¦ä¹ ä»»åŠ¡
- æ”¶æ•›é€Ÿåº¦å¿«

**æ¨èå­¦ä¹ ç‡ï¼š** 0.0001 ~ 0.01

#### 3. RMSprop

```cpp
modeldy::RMSprop<float> optimizer(
    0.01f,      // learning_rate
    0.99f,      // alpha (è¡°å‡ç‡ï¼Œå¯é€‰)
    1e-8f,      // epsilon (å¯é€‰)
    0.0f        // weight_decay (å¯é€‰)
);
```

**ç‰¹ç‚¹ï¼š**
- é€‚åˆå¤„ç†éå¹³ç¨³ç›®æ ‡
- é€‚åˆ RNN è®­ç»ƒ
- è‡ªé€‚åº”å­¦ä¹ ç‡

**æ¨èå­¦ä¹ ç‡ï¼š** 0.001 ~ 0.01

---

## è®­ç»ƒæµç¨‹

### å®Œæ•´è®­ç»ƒç¤ºä¾‹

```cpp
#include <modeldy/include/model.h>
#include <modeldy/include/optimizer.h>
#include <modeldy/include/operator_registry.h>

int main() {
    // 1. åˆ›å»ºæ¨¡å‹
    modeldy::Model<float> model;
    
    // 2. å®šä¹‰ç½‘ç»œç»“æ„
    model.newDataNode("input", {batch_size, input_dim}, false, "cpu");
    model.newDataNode("weights", {input_dim, output_dim}, true, "cpu");  // å¯è®­ç»ƒå‚æ•°
    model.newDataNode("bias", {output_dim}, true, "cpu");                // å¯è®­ç»ƒå‚æ•°
    model.newDataNode("output", {batch_size, output_dim}, true, "cpu");
    model.newDataNode("target", {batch_size, output_dim}, false, "cpu");
    model.newDataNode("loss", {1}, true, "cpu");
    
    // 3. åˆå§‹åŒ–å‚æ•°
    model.setData("weights", initial_weights);
    model.setData("bias", initial_bias);
    
    // 4. æ ‡è®°å¯è®­ç»ƒå‚æ•°
    model.add_parameter("weights");
    model.add_parameter("bias");
    
    // 5. æ„å»ºè®¡ç®—å›¾
    model.newComputeNode("GemmOO", "linear", {"input", "weights"}, {"temp"}, "cpu");
    model.newComputeNode("Add", "add_bias", {"temp", "bias"}, {"output"}, "cpu");
    model.newComputeNode("MSELoss", "loss_fn", {"output", "target"}, {"loss"}, "cpu");
    
    // 6. åˆ›å»ºä¼˜åŒ–å™¨å¹¶å…³è”å‚æ•°
    modeldy::Adam<float> optimizer(0.001f);
    model.setup_optimizer(optimizer);
    
    // 7. è®­ç»ƒå¾ªç¯
    for (int epoch = 0; epoch < num_epochs; ++epoch) {
        // è®¾ç½®è¾“å…¥æ•°æ®
        model.setData("input", batch_input);
        model.setData("target", batch_target);
        
        // å•æ­¥è®­ç»ƒ
        float loss = model.train_step(optimizer, "loss", true);
        
        std::cout << "Epoch " << epoch << ", Loss: " << loss << std::endl;
    }
    
    // æˆ–è€…ä½¿ç”¨æ‰¹é‡è®­ç»ƒ
    auto losses = model.train(optimizer, "loss", 100, 10);  // 100æ¬¡è¿­ä»£ï¼Œæ¯10æ¬¡æ‰“å°
    
    return 0;
}
```

---

## API è¯´æ˜

### Model ç±»è®­ç»ƒç›¸å…³æ–¹æ³•

#### add_parameter()
```cpp
void add_parameter(const std::string& name)
```

æ ‡è®°ä¸€ä¸ªæ•°æ®èŠ‚ç‚¹ä¸ºå¯è®­ç»ƒå‚æ•°ã€‚

**å‚æ•°ï¼š**
- `name`: å‚æ•°èŠ‚ç‚¹çš„åç§°

**è¦æ±‚ï¼š**
- èŠ‚ç‚¹å¿…é¡»æ˜¯ DataNode
- èŠ‚ç‚¹å¿…é¡»è®¾ç½® `requires_grad=true`

#### setup_optimizer()
```cpp
void setup_optimizer(Optimizer<T>& optimizer)
```

å°†æ¨¡å‹çš„æ‰€æœ‰å¯è®­ç»ƒå‚æ•°æ³¨å†Œåˆ°ä¼˜åŒ–å™¨ã€‚

**å‚æ•°ï¼š**
- `optimizer`: ä¼˜åŒ–å™¨å®ä¾‹çš„å¼•ç”¨

#### train_step()
```cpp
T train_step(Optimizer<T>& optimizer, const std::string& loss_node_name, bool verbose = false)
```

æ‰§è¡Œä¸€æ¬¡è®­ç»ƒè¿­ä»£ï¼ˆå‰å‘ä¼ æ’­ã€åå‘ä¼ æ’­ã€å‚æ•°æ›´æ–°ï¼‰ã€‚

**å‚æ•°ï¼š**
- `optimizer`: ä¼˜åŒ–å™¨
- `loss_node_name`: æŸå¤±èŠ‚ç‚¹çš„åç§°
- `verbose`: æ˜¯å¦æ‰“å°æŸå¤±å€¼

**è¿”å›ï¼š**
- å½“å‰è¿­ä»£çš„æŸå¤±å€¼

#### train()
```cpp
std::vector<T> train(Optimizer<T>& optimizer,
                     const std::string& loss_node_name,
                     size_t num_iterations,
                     size_t print_every = 0)
```

æ‰§è¡Œå¤šæ¬¡è®­ç»ƒè¿­ä»£ã€‚

**å‚æ•°ï¼š**
- `optimizer`: ä¼˜åŒ–å™¨
- `loss_node_name`: æŸå¤±èŠ‚ç‚¹çš„åç§°
- `num_iterations`: è®­ç»ƒè¿­ä»£æ¬¡æ•°
- `print_every`: æ¯ N æ¬¡è¿­ä»£æ‰“å°ä¸€æ¬¡ï¼ˆ0 è¡¨ç¤ºä¸æ‰“å°ï¼‰

**è¿”å›ï¼š**
- æ¯æ¬¡è¿­ä»£çš„æŸå¤±å€¼å‘é‡

### Optimizer ç±»æ–¹æ³•

#### step()
```cpp
virtual void step() = 0
```

æ ¹æ®æ¢¯åº¦æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚

#### zero_grad()
```cpp
virtual void zero_grad()
```

å°†æ‰€æœ‰å‚æ•°çš„æ¢¯åº¦æ¸…é›¶ã€‚

#### learning_rate() / set_learning_rate()
```cpp
T learning_rate() const
void set_learning_rate(T lr)
```

è·å–æˆ–è®¾ç½®å­¦ä¹ ç‡ã€‚

---

## é«˜çº§ç”¨æ³•

### å­¦ä¹ ç‡è°ƒåº¦

```cpp
modeldy::Adam<float> optimizer(0.1f);
model.setup_optimizer(optimizer);

// è®­ç»ƒå‰æœŸ
model.train(optimizer, "loss", 50);

// é™ä½å­¦ä¹ ç‡
optimizer.set_learning_rate(0.01f);

// ç»§ç»­è®­ç»ƒ
model.train(optimizer, "loss", 50);
```

### ä½¿ç”¨åŠ¨é‡

```cpp
// SGD with momentum
modeldy::SGD<float> optimizer(
    0.01f,      // learning_rate
    0.9f,       // momentum
    0.0f        // weight_decay
);
```

### L2 æ­£åˆ™åŒ–

```cpp
// ä½¿ç”¨æƒé‡è¡°å‡è¿›è¡ŒL2æ­£åˆ™åŒ–
modeldy::Adam<float> optimizer(
    0.001f,     // learning_rate
    0.9f,       // beta1
    0.999f,     // beta2
    1e-8f,      // epsilon
    0.01f       // weight_decay (L2 regularization)
);
```

### è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯

```cpp
modeldy::SGD<float> optimizer(0.01f);
model.setup_optimizer(optimizer);

for (int epoch = 0; epoch < num_epochs; ++epoch) {
    float total_loss = 0.0f;
    
    for (int batch = 0; batch < num_batches; ++batch) {
        // åŠ è½½æ‰¹æ¬¡æ•°æ®
        model.setData("input", batch_input[batch]);
        model.setData("target", batch_target[batch]);
        
        // å‰å‘ä¼ æ’­
        model.predict();
        const float* loss_data = model.data("loss");
        total_loss += loss_data[0];
        
        // åå‘ä¼ æ’­
        optimizer.zero_grad();
        model.backward("loss");
        
        // æ›´æ–°å‚æ•°
        optimizer.step();
    }
    
    std::cout << "Epoch " << epoch 
              << ", Average Loss: " << total_loss / num_batches << std::endl;
}
```

---

## CPU/CUDA åŒå®ç°æ¶æ„

### æ¶æ„è®¾è®¡

ä¼˜åŒ–å™¨ç³»ç»Ÿæ”¯æŒ CPU å’Œ CUDA ä¸¤ç§å®ç°ï¼Œèƒ½å¤Ÿè‡ªåŠ¨æ£€æµ‹å‚æ•°ç±»å‹å¹¶è°ƒç”¨ç›¸åº”çš„å®ç°ã€‚

#### 1. è‡ªåŠ¨è®¾å¤‡æ£€æµ‹

ä¼˜åŒ–å™¨ä¼šè‡ªåŠ¨æ£€æµ‹å‚æ•°æ˜¯ `cpuDataNode` è¿˜æ˜¯ `cudaDataNode`ï¼Œå¹¶è°ƒç”¨ç›¸åº”çš„å®ç°ï¼š

```cpp
void step() override {
  for (auto& param : this->parameters_) {
    if (param.is_cuda) {
      step_cuda(param);  // CUDA å®ç°
    } else {
      step_cpu(param);   // CPU å®ç°
    }
  }
}
```

#### 2. åˆ†ç¦»çš„å®ç°

æ¯ä¸ªä¼˜åŒ–å™¨ç±»éƒ½åŒ…å«ä¸¤ä¸ªç‹¬ç«‹çš„å®ç°æ–¹æ³•ï¼š
- `step_cpu()` - CPU å®ç°ï¼ˆå·²å®Œæˆï¼‰
- `step_cuda()` - CUDA å®ç°ï¼ˆæ¡†æ¶å·²å°±ç»ªï¼Œéœ€è¦å®ç° kernelï¼‰

### æ–‡ä»¶ç»“æ„

```
include/
â”œâ”€â”€ optimizer.h                    # ä¼˜åŒ–å™¨ä¸»æ–‡ä»¶
â””â”€â”€ cuda/
    â””â”€â”€ optimizer_kernels.h        # CUDA kernel å£°æ˜å’Œå¯åŠ¨å‡½æ•°
```

### ä½¿ç”¨ç¤ºä¾‹

#### CPU è®­ç»ƒ
```cpp
modeldy::Model<float> model;
// ... è®¾ç½® CPU èŠ‚ç‚¹ ...

modeldy::Adam<float> optimizer(0.001f);
model.setup_optimizer(optimizer);

// è‡ªåŠ¨ä½¿ç”¨ CPU å®ç°
model.train(optimizer, "loss", 100);
```

#### CUDA è®­ç»ƒ
```cpp
#ifdef USE_CUDA
modeldy::Model<float> model;
// ... è®¾ç½® CUDA èŠ‚ç‚¹ ...

modeldy::Adam<float> optimizer(0.001f);
model.setup_optimizer(optimizer);

// è‡ªåŠ¨ä½¿ç”¨ CUDA å®ç°
model.train(optimizer, "loss", 100);
#endif
```

#### æ··åˆæ¨¡å¼
ç³»ç»Ÿæ”¯æŒæ··åˆ CPU å’Œ CUDA å‚æ•°ï¼Œæ¯ä¸ªå‚æ•°ä¼šè‡ªåŠ¨ä½¿ç”¨å¯¹åº”çš„å®ç°ã€‚

---

## CUDA Kernel å®ç°æŒ‡å—

### éœ€è¦å®ç°çš„ CUDA Kernels

åœ¨ `include/cuda/optimizer_kernels.h` ä¸­ï¼Œå·²æä¾›ä»¥ä¸‹ kernel çš„æ¡†æ¶ï¼š

#### 1. SGD Kernels

```cuda
// æ— åŠ¨é‡ç‰ˆæœ¬
template <typename T>
__global__ void sgd_kernel(T* data, const T* grad, size_t size, 
                          T lr, T weight_decay)

// å¸¦åŠ¨é‡ç‰ˆæœ¬
template <typename T>
__global__ void sgd_momentum_kernel(T* data, const T* grad, T* velocity,
                                   size_t size, T lr, T momentum, T weight_decay)
```

**å…¬å¼ï¼š**
- æ— åŠ¨é‡: `data = data - lr * (grad + weight_decay * data)`
- æœ‰åŠ¨é‡: 
  ```
  velocity = momentum * velocity + (grad + weight_decay * data)
  data = data - lr * velocity
  ```

#### 2. Adam Kernel

```cuda
template <typename T>
__global__ void adam_kernel(T* data, const T* grad, T* m, T* v,
                           size_t size, T lr, T beta1, T beta2,
                           T epsilon, T weight_decay, size_t t)
```

**å…¬å¼ï¼š**
```
g = grad + weight_decay * data
m = beta1 * m + (1 - beta1) * g
v = beta2 * v + (1 - beta2) * g^2
m_hat = m / (1 - beta1^t)
v_hat = v / (1 - beta2^t)
data = data - lr * m_hat / (sqrt(v_hat) + epsilon)
```

#### 3. RMSprop Kernel

```cuda
template <typename T>
__global__ void rmsprop_kernel(T* data, const T* grad, T* square_avg,
                              size_t size, T lr, T alpha,
                              T epsilon, T weight_decay)
```

**å…¬å¼ï¼š**
```
g = grad + weight_decay * data
square_avg = alpha * square_avg + (1 - alpha) * g^2
data = data - lr * g / (sqrt(square_avg) + epsilon)
```

### å®ç°æ­¥éª¤

#### 1. å–æ¶ˆæ³¨é‡Š kernel ä»£ç 

åœ¨ `include/cuda/optimizer_kernels.h` ä¸­ï¼Œæ¯ä¸ª kernel å‡½æ•°ä½“éƒ½æœ‰æ³¨é‡Šçš„å®ç°ä»£ç ï¼Œå–æ¶ˆæ³¨é‡Šå³å¯ï¼š

```cuda
template <typename T>
__global__ void sgd_kernel(T* data, const T* grad, size_t size, T lr, T weight_decay) {
  // å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  if (idx < size) {
    T g = grad[idx];
    if (weight_decay > 0) {
      g += weight_decay * data[idx];
    }
    data[idx] -= lr * g;
  }
}
```

#### 2. å–æ¶ˆæ³¨é‡Š kernel å¯åŠ¨å‡½æ•°

```cuda
template <typename T>
void sgd_kernel_launch(T* data, const T* grad, size_t size, T lr, T weight_decay) {
  // å–æ¶ˆä¸‹é¢çš„æ³¨é‡Š
  int block_size = 256;
  int grid_size = (size + block_size - 1) / block_size;
  sgd_kernel<<<grid_size, block_size>>>(data, grad, size, lr, weight_decay);
  CUDA_CHECK(cudaGetLastError());
}
```

#### 3. åˆ›å»º .cu æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰

å¦‚æœéœ€è¦å°† kernel å®ç°åˆ†ç¦»åˆ° .cu æ–‡ä»¶ï¼š

```cuda
// src/cuda/optimizer_kernels.cu
#include <modeldy/include/cuda/optimizer_kernels.h>

namespace modeldy {
namespace cuda {

// å®ç°æ‰€æœ‰ kernel å’Œå¯åŠ¨å‡½æ•°
// ...

} // namespace cuda
} // namespace modeldy
```

### å†…å­˜ç®¡ç†

#### CPU å®ç°
- ä½¿ç”¨ `std::vector<T>` å­˜å‚¨è¾…åŠ©å˜é‡ï¼ˆvelocity, momentum ç­‰ï¼‰
- è‡ªåŠ¨ç®¡ç†å†…å­˜

#### CUDA å®ç°
- ä½¿ç”¨ `T*` è®¾å¤‡æŒ‡é’ˆå­˜å‚¨è¾…åŠ©å˜é‡
- åœ¨ç¬¬ä¸€æ¬¡ä½¿ç”¨æ—¶åˆ†é…ï¼š
  ```cpp
  CUDA_CHECK(cudaMalloc(&velocity_ptr, total_size * sizeof(T)));
  CUDA_CHECK(cudaMemset(velocity_ptr, 0, total_size * sizeof(T)));
  ```
- å»ºè®®åœ¨ææ„å‡½æ•°ä¸­é‡Šæ”¾å†…å­˜

---

## ä¼˜åŒ–å»ºè®®

### CUDA Kernel ä¼˜åŒ–æŠ€å·§

#### 1. çº¿ç¨‹å—å¤§å°
- æ¨è 256 æˆ– 512
- æ ¹æ®å¯„å­˜å™¨ä½¿ç”¨æƒ…å†µè°ƒæ•´
- ä½¿ç”¨ occupancy calculator ç¡®å®šæœ€ä¼˜å€¼

#### 2. å†…å­˜è®¿é—®
- ç¡®ä¿åˆå¹¶è®¿é—®ï¼ˆcoalesced accessï¼‰
- ä½¿ç”¨ shared memory ä¼˜åŒ–ï¼ˆé«˜çº§ï¼‰
- é¿å… bank conflicts

#### 3. æ•°å€¼ç¨³å®šæ€§
- æ³¨æ„é™¤é›¶æ£€æŸ¥
- ä½¿ç”¨ `rsqrtf()` ä»£æ›¿ `1.0f / sqrtf()`
- ä½¿ç”¨ FMA (fused multiply-add) æŒ‡ä»¤

#### 4. ä¼˜åŒ–ç¤ºä¾‹

```cuda
__global__ void sgd_kernel_optimized(T* data, const T* grad, 
                                    size_t size, T lr, T weight_decay) {
  int idx = blockIdx.x * blockDim.x + threadIdx.x;
  int stride = blockDim.x * gridDim.x;
  
  // Grid-stride loop for better workload distribution
  for (int i = idx; i < size; i += stride) {
    T g = grad[i];
    if (weight_decay > 0) {
      g = fmaf(weight_decay, data[i], g);  // ä½¿ç”¨ FMA
    }
    data[i] = fmaf(-lr, g, data[i]);  // ä½¿ç”¨ FMA
  }
}
```

### æ€§èƒ½æç¤º

1. **æ‰¹é‡å¤§å°**ï¼šå¢å¤§æ‰¹é‡å¤§å°ä»¥æé«˜ GPU åˆ©ç”¨ç‡
2. **å¼‚æ­¥æ“ä½œ**ï¼šä½¿ç”¨ CUDA streams è¿›è¡Œå¼‚æ­¥è®¡ç®—
3. **æ•°æ®ä¼ è¾“**ï¼šæœ€å°åŒ– CPU-GPU æ•°æ®ä¼ è¾“
4. **æ··åˆç²¾åº¦**ï¼šè€ƒè™‘ä½¿ç”¨ FP16 åŠ é€Ÿè®­ç»ƒï¼ˆéœ€è¦é¢å¤–å®ç°ï¼‰

---

## ç¼–è¯‘ä¸æµ‹è¯•

### CPU æ¨¡å¼ç¼–è¯‘

```powershell
# ä» Desktop/modeldy ç›®å½•
g++ -std=c++17 -I. modeldy/examples/training_example.cpp -o training_example.exe
.\training_example.exe
```

### CUDA æ¨¡å¼ç¼–è¯‘

```bash
# ç¼–è¯‘ CUDA kernels
nvcc -c src/cuda/optimizer_kernels.cu -o optimizer_kernels.o -DUSE_CUDA

# é“¾æ¥æœ€ç»ˆç¨‹åº
nvcc -std=c++17 -I. modeldy/examples/training_example.cpp optimizer_kernels.o -o training_example -DUSE_CUDA

# è¿è¡Œ
.\training_example
```

### æµ‹è¯•ç”¨ä¾‹

è¿è¡Œæ¢¯åº¦æµ‹è¯•ä»¥éªŒè¯å®ç°çš„æ­£ç¡®æ€§ï¼š

```powershell
cd Desktop/modeldy
g++ -std=c++17 -I. modeldy/examples/loss_gradient_test.cpp -o loss_gradient_test.exe
.\loss_gradient_test.exe
```

---

## æ³¨æ„äº‹é¡¹

### é€šç”¨æ³¨æ„äº‹é¡¹

1. **æ¢¯åº¦è®¾ç½®**ï¼šæ‰€æœ‰å‚ä¸åå‘ä¼ æ’­çš„ä¸­é—´èŠ‚ç‚¹éƒ½éœ€è¦è®¾ç½® `requires_grad=true`
2. **å‚æ•°åˆå§‹åŒ–**ï¼šè®­ç»ƒå‰è¦åˆç†åˆå§‹åŒ–å‚æ•°ï¼Œé¿å…æ¢¯åº¦æ¶ˆå¤±æˆ–çˆ†ç‚¸
3. **å†…å­˜ç®¡ç†**ï¼šæ¯æ¬¡è¿­ä»£å‰ç¡®ä¿æ•°æ®å·²æ­£ç¡®åŠ è½½
4. **æ•°å€¼ç¨³å®šæ€§**ï¼šæŸå¤±å‡½æ•°å’Œæ¿€æ´»å‡½æ•°å†…éƒ¨å·²åŒ…å«æ•°å€¼ç¨³å®šæ€§å¤„ç†

### CUDA ç‰¹å®šæ³¨æ„äº‹é¡¹

1. **è®¾å¤‡åŒæ­¥**ï¼šåœ¨è¯»å–ç»“æœå‰ç¡®ä¿ kernel æ‰§è¡Œå®Œæˆ
2. **é”™è¯¯æ£€æŸ¥**ï¼šä½¿ç”¨ `CUDA_CHECK` å®æ£€æŸ¥æ‰€æœ‰ CUDA è°ƒç”¨
3. **å†…å­˜æ³„æ¼**ï¼šç¡®ä¿æ­£ç¡®é‡Šæ”¾åˆ†é…çš„è®¾å¤‡å†…å­˜
4. **è®¡ç®—èƒ½åŠ›**ï¼šç¡®è®¤ GPU æ”¯æŒæ‰€éœ€çš„ CUDA è®¡ç®—èƒ½åŠ›

---

## çŠ¶æ€æ€»ç»“

### âœ… å·²å®Œæˆ

- CPU å®ç°ï¼ˆSGD, Adam, RMSpropï¼‰
- è®­ç»ƒæ¥å£å’Œ API
- è‡ªåŠ¨è®¾å¤‡æ£€æµ‹æœºåˆ¶
- CUDA æ¡†æ¶å’Œæ¥å£
- å†…å­˜ç®¡ç†æ¡†æ¶
- å®Œæ•´çš„æ–‡æ¡£å’Œç¤ºä¾‹

### â³ å¾…å®ç°

- CUDA kernel å…·ä½“å®ç°ï¼ˆæ¡†æ¶å’Œå…¬å¼å·²æä¾›ï¼‰
- CUDA kernel æ€§èƒ½ä¼˜åŒ–
- æ··åˆç²¾åº¦æ”¯æŒï¼ˆå¯é€‰ï¼‰
- åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼ˆå¯é€‰ï¼‰

---

## ç¤ºä¾‹è¾“å‡º

```
=== Training Simple Network with Different Optimizers ===

--- Using SGD ---
Initial weight: 0.1, Target: 5.0
Iteration 5/20, Loss: 4.957568
Iteration 10/20, Loss: 4.346873
Iteration 15/20, Loss: 23.644493
Iteration 20/20, Loss: 16.507284
Final weight: 9.947211
Expected: ~5.0

--- Using Adam ---
Initial weight: 0.1, Target: 5.0
Iteration 5/20, Loss: 20.281775
Iteration 10/20, Loss: 15.998478
Iteration 15/20, Loss: 12.053538
Iteration 20/20, Loss: 8.512946
Final weight: 2.196091
Expected: ~5.0
```

---

## å‚è€ƒèµ„æº

- **é¡¹ç›®ç¤ºä¾‹**ï¼š`examples/training_example.cpp`
- **æ¢¯åº¦æµ‹è¯•**ï¼š`examples/loss_gradient_test.cpp`
- **ä¼˜åŒ–å™¨å¤´æ–‡ä»¶**ï¼š`include/optimizer.h`
- **CUDA kernel æ¡†æ¶**ï¼š`include/cuda/optimizer_kernels.h`

---

**æœ€åæ›´æ–°ï¼š** 2026-02-02  
**ç‰ˆæœ¬ï¼š** 1.0
